robots.txt 
각 사이트의 크롤러(네이버는 Yeti)가 크롤링하여 웹사이트의 정보를 수집하는데 아무사이트의 정보를 막 긁어오면 공개를 원치않는 사이트까지 가져오게되는 문제가 발생한다. 
그래서 robots.txt 파일에서 크롤링이 가능한지 여부를 확인한 후 가능하다면 크롤링하도록 규정함.

sitemap.xml
말 그대로 사이트맵이다. 크롤러가 사이트를 보다 쉽게 찾을 수 있도록 도와준다.
구글에 'sitemap generator'를 검색하여 생성하면 된다.

네이버에 내 사이트 검색되도록하려면 '네이버 서치어드바이저' 들어가보자.
